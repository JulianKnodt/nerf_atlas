\section*{Method}

Our design imposes additional structure on top of existing machine learning
approaches, enforcing properties on the reconstructed values. For functions \[ f(p, t)\to\mathbb{R}^n, t\in[0,1] \] which is modelled through
a learned approach, we decompose it into a function: \[ f(p)\to\mathbb{R}^{n\times O}, B_O(f(p), t)\to\mathbb{R}^n
\] Where
$O$ is the order of the Bezier spline, $f(p)$ is the learned control points, and $B_O(f(p), \cdot)$ is the evaluation
of the $O$th order Bezier spline with control points defined by $f(p)$.

\subsection*{Architecture}

Specifically for dynamic NeRF, $f(p)$ is defined as
$\text{MLP}(x,y,z)\to\mathbb{R}^{3\times O}$. We ray march from a camera with known position and
view direction through the scene, and at every point we compute the set of control points for a Bezier curve. We then evaluate the Bezier curve at a given time, and deform the ray by the result, producing some $\Delta_0(x)$. The number of spline points for the Bezier curve is a  hyperparameter, but we are able to accurately reconstruct with 4 spline points, and experiment with up to 6 spline points. In order to evaluate the Bezier curve in a numerically stable way, we use De Casteljau's algorithm. 

De Casteljau's algorithm evaluated at time $t$ is defined by the recurrence relation:
\begin{align*}
  \beta_i^{(0)} &= \beta_i \nonumber \\
  \beta_i^{j} &= (1-t)\beta_i^{(j-1)} + t\beta_{i-1}^{(j-1)} \nonumber \\
\end{align*}
which can be thought of as linearly interpolating between adjacent control points until there is only a single point remaining. This takes $O(n^2)$ operations to evaluate, where $n$ is the number of control points. For a small $n$, this is negligible.

We are also interested in constructing a reasonable canonical NeRF, and without loss of generality select $t = 0$ to be canonical. From this, we are interested in Bezier Curves where $B_O(0) = \overrightarrow{0}$. This can be achieved in two different ways, either by assigning $p_0 = \overrightarrow{0}$, and only computing the other control points: $f(p) = p_{1\cdots O-1}$. Then, we can use the Bezier spline with the control points as the concatenation of $\overrightarrow{0}$ with the other control points: $[\overrightarrow{0}, p_{1\cdots O-1}]$. Alternatively, we can compute $p_{0\cdots O-1}$ and use the Bezier spline with control points but subtract the first predicted point from all of them: $[p_{0\cdots O-1}]-p_0$, and the final change in position is $\Delta_0(x) = B_O(f(p)-f(p)_0,t)+p_0$. While both formulations are equivalent and explicitly concatenating $\overrightarrow{0}$ leads to fewer degrees of freedom, we find the second approach leads to better convergence near $t=0$, so we use that. This is likely because this has a higher degree of freedom at $t=0$. We note that rigidity is not applied to the first control point: $\delta'(x,y,z) = r\delta(x,y,z) + p_0$.

A full diagram of the ray-bending component can be seen in Fig.~\ref{fig:arch_diagram}.

Following NR-NeRF, we also learn how rigid each ray is, which should allow more efficient learning of which regions are space are fixed. This rigidity $\in[\epsilon, 1]$)is computed as a function of position:
\begin{align*}
  \text{r} &=\sigma_{\epsilon^\uparrow}(\text{MLP}(x)) \nonumber \\
  \sigma_{\epsilon^\uparrow}(v) &= \sigma(v)(1-\epsilon) + \epsilon \nonumber \\
\end{align*}
where $\sigma$ is defined as the sigmoid function $\frac{1}{1+e^{-x}}$. Rigidity $\in[0,1]$ rescales the
difficulty of learning movement, making it more easy to represent and classify static scene
objects. The final change in position is defined as $\Delta(x) = \text{Rigidity}\times
\Delta_0(x)$.

In order to reconstruct RGB values, we also diverge from the original NeRF and NR-NeRF. Instead of only allowing for fully positional or view-dependent colors, RGB is mostly a function of position, but we allow a small amount of linear scaling as a function of the view direction.
\begin{align}
  \text{RGB}_0 &= \sigma(\text{MLP}(x)) \nonumber \\
  \text{RGB} &= (1-\text{MLP}(v)/2)\text{RGB}_0 \nonumber
\end{align}
Because of the low number of
samples for a moving object at a given view, it is more difficult to learn specular
reflection, but it is often the case there are often simple lighting changes which are easier to model. Motivation for this modification can be found under limitations(\ref{sec:refl_disc}).

\subsection*{Training}

For training, we sample random crops of random frames, computing the loss defined previously and back-propagating through the whole network. We use autograd to optimize control points and the canonical NeRF jointly, but note that there are also classical approaches to solving spline points which may lead to faster optimization in the future. We use the Adam optimizer~\cite{Kingma2015AdamAM} with cosine simulated annealing~\cite{loshchilov2017sgdr} to go from $\num{2e-4}$ to $\num{5e-5}$ over the course of a day.

For some scenes, we are able to have higher learning rates, but for much darker scenes it's necessary to lower the learning rate to converge.

We additionally apply the many optimizations introduced in NR-NeRF, including rigidity:

\begin{align}\label{eq:rigidity_defn}
    \text{r} = \sigma(\text{MLP}(x,y,z)) \\
    \Delta'(x,y,z) = \text{r}\Delta(x,y,z)
\end{align}

Which effectively marks certain components of the scene as static, 0 rigidity corresponding to no movement across time steps.

We also find it crucial to apply offset and divergence regularization.
\begin{align}
\ell_{\text{offsets}} = \frac{1}{|C|} \sum_{j\in C} \alpha_j \
    (\lVert \Delta(x,y,z) \rVert_2^{2-\text{r}}+\lambda_r \text{r})
\end{align}
\begin{align}
    \ell_{\text{divergence}} = \frac{1}{|C|} \sum_{j\in C} \omega_j' |\nabla\cdot(\Delta'(x_j,y_j,z_j))|^2
\end{align}

Where $r$ is rigidity as defined in Eq.\ref{eq:rigidity_defn}, and $\lambda_r$ is a hyper-parameter, set to 0.3, $\Delta(x,y,z), \Delta'(x,y,z)$ is the change in position before and after the rigidity is applied respectively, and $\alpha_j$ refers to the accumulated visibility weight along a ray.

We defer to NR-NeRF~\cite{tretschk2021nonrigid} for a complete explanation of these losses.

\subsection{Voxel Spline-NeRF}

In addition to a model that uses an MLP to predict the control points, we demonstrate that using control points is also effective for voxelized NeRF, which leads to much faster reconstruction times. Our formulation is identical to the MLP model, but instead of querying an MLP, the model trilinearly interpolates between the surrounding set of voxel coordinates. We demonstrate the efficacy of using a voxelized approach for reconstructing dynamic scenes at high quality in our experiments, but do not demonstrate precisely how much faster it is than the MLP based approach, since it is heavily implementation-dependent, and a voxel based approach would do well from using a handwritten CUDA extension, while our implementation is currently only written in normal Pytorch. We do note that training is significantly faster and much less memory-intensive than the MLP based approach, allowing for an order of magnitude higher batch size while training, and converging significantly faster.

A diagram explaining the voxel implementation is shown in Fig.~\ref{fig:voxel_diagram}. It closely resembles the MLP based approach, only differing in that it must store a set of spline control points at every voxel position. A simple implementation also does not differ significantly from a standard dynamic NeRF approach, differing by around 50 LOC.

We note that the losses introduced in NR-NeRF~\cite{tretschk2021nonrigid} are crucial to the convergence of voxel Spline-NeRF. We also find it necessary to apply total variational loss as introduced in Plenoxels~\cite{yu2021plenoxels}:

\begin{align}
    \ell_{TV} = \frac{1}{|V|} \sum_{v\in V} \sqrt{\sum_{d\in \{x,y,z\}} \Delta^2_d(v)}
\end{align}

Where $\Delta^2_d(v)$ is the difference between one of a voxel's value and one of its neighbor on the $d$ axis's corresponding values. This guarantees that there is relative consistency in the voxel grid. We note that we apply this to all components stored in the voxel grid, including the spline control points, the rigidity, the density, and the RGB. As in Plenoxels, we stochastically sample this at each step.