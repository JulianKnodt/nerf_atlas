\section*{Method}

Our method consists of imposing additional structure on top of existing machine learning
approaches to reconstruction. For some function $f(p, t)\to\mathbb{R}^n, t\in[0,1]$ which is modelled through
some learned approach, we decompose it into a function $f(p)\to\mathbb{R}^{n\times O}, B_O(f(p), t)$, where
$O$ is the order of the Bezier spline, $f$ is the learned control points, and $B_O$ is the evaluation
of the $O$'th order Bezier spline with control points defined by $f(p)$.

\subsection*{Architecture}

Specifically for the case of dynamic NeRF, we formulate $f(p)$ as
$\text{MLP}(x,y,z)\to\mathbb{R}^{3\times O}$. We raymarch from a camera with known position and
view direction through the scene, and at every point we compute the set of control points for
the amount the ray is being bent. We then are able to evaluate it at any time $t=[0,1]$ in order
to reconstruct movement, $\Delta_0(x)$. The number of spline points is variable, but we are able
to reconstruct movement with as low as 4 spline points, but experiment with at least 16 spline
points. In order to evaluate the Bezier curve in a numerically stable way, we use De Casteljau's
algorithm, and further plan on extending it to handle rational bezier splines, which would allow
for even greater control of movement.

De Casteljau's algorithm evaluated at time $t$ is characterized by the recurrence relation:
\[ \beta_i^{(0)} = \beta_i \]
\[ \beta_i^{j} = (1-t)\beta_i^{(j-1)} + t\beta_{i-1}^{(j-1)} \]
Which can be conceptually thought of as linearly interpolating between adjacent control points until there is only a single point remaining. This takes $O(n^2)$ operations to evaluate, where $n$ is the number of control points.

In addition, we are also interested in constructing a reasonable canonical NeRF, and arbitrarily select $t = 0$ to be the canonical NeRF. Thus, we are interested in Bezier Curves where $B_O(0) = \overrightarrow{0}$. This can be achieved in two different ways, either by assigning $p_0 = \overrightarrow{0}$, and only compute $f(p) = p_{1\cdots O-1}$. Then, we can use the Bezier spline with the control points as $[\overrightarrow{0}, p_{1\cdots O-1}]$. Alternatively, we can compute $p_{0\cdots O-1}$ and use the Bezier spline with control points $[p_{0\cdots O-1}]-p_0$, and the final change in position is $B_O(f(p)-f(p)_0,t)+p_0$. While both formulations are in theory equivalent, and explicitly concatenating 0 leads to fewer degrees of freedom, we find the second one to lead to better convergence near 0, so we use that in our models.

Following NR-NeRF, we also apply a rigidity to every ray, which is also computed as a function of position:
\[ \text{Rigidity}\in[\epsilon, 1] =\sigma_{\epsilon^\uparrow}(\text{MLP}(x,y,z)) \]
\[ \sigma_{\epsilon^\uparrow}(v) = \sigma(v)(1-\epsilon) + \epsilon \]
Which essentially rescales the
difficulty of learning movement, making it more easy to represent and classify static scene
objects. The final change in position is thus defined as $\Delta(x) = \text{Rigidity}\times
\Delta_0(x)$.

In order to reconstruct RGB values, we also diverge from traditional NeRF, and only allow for
positional dependent colors:
\[ \text{RGB} = \sigma_{\epsilon^\Leftrightarrow}(\text{MLP}(x,y,z)) \]
\[ \sigma_{\epsilon^\Leftrightarrow}(v) = \sigma(v)(1+2\epsilon) - \epsilon \]
Using the
activation function as described in MIP-NeRF~\cite{barron2021mipnerf} in order to better reconstruct colors. Because of the low number of
samples for a moving object at a given view, it is much more difficult to learn specular
reflection, thus it becomes necessary to only model the Lambertian component of the color. This is in line with NR-NeRF and D-NeRF, which models the diffuse component.

\subsection*{Loss}
While training, we also introduce an additional loss term, as the $\ell_2$ loss may have
difficulty when there is a large pixel-wise gap in movement between the learned and predicted
component. We formulate the loss term as $\text{MSE}(\text{FFT}(I_\text{GT}),
\text{FFT}(I_\text{predicted}))$, where the FFT is the 2D fast fourier transform of an image.
The MSE here is defined over the complex domain as the FFT has both a real and imaginary
component, and is defined as \[ L_\text{FFT} =\text{MSE}(a\in\mathbb{C}^k,b\in\mathbb{C}^k) \]
\[ = \frac{1}{k}\Sigma_{i=0}^k|a_i-b_i| \]
\[ |z\in\mathbb{C}| = (\Re(z)+\Im(z))(\Re(z)-\Im(z)) \]
We introduce this term as a cheap replacement to structural similarity, since the FFT captures
information about the structure of the whole image, which is useful in a dynamic setting due to
the disjointedness of where a predicted object may be as opposed to its final position\footnote{In a published iteration, we would likely perform an ablation of this loss function.}.

Our final loss formulation is thus:
\[ \ell_2(I_\text{predicted}, I_\text{GT}) + L_\text{FFT}(I_\text{predicted}, I_\text{GT}) \]
without any additional regularization terms.

Our formulation is simpler compared to NR-NeRF since there is no need to regularize over
temporal consistency, and that makes the optimization process simpler. Our training process also does not require adding in additional frames over time for relatively short sequences, so we randomly sample from the set of frames. This is contrast to D-NeRF, which requires adding frames in during training since it has to enforce that at $t=0$ the predicted movement is 0.

\subsection*{Training}

For training, we sample random crops of given frames, computing L2 and FFT loss and back-propagating through the whole network. We use autograd to optimize the splines, but note that there are also classical approaches to solving them which may lead to faster optimization in the future. We use the Adam optimizer~\cite{Kingma2015AdamAM} with cosine simulated annealing~\cite{loshchilov2017sgdr} to go from $\num{2e-4}$ to $\num{5e-5}$ over the course of a day.

In order to ensure that our model is robust to small changes in camera angle, we introduce subpixel jitter in the camera, identical to NeRF. We dot not introduce any jitter in the time-domain, since we are guaranteed smoothness from our formulation.

\subsection*{Long Duration $C^0$ Continuity}

While the above structure is sufficient to model short-term $C^0$ continuity, it runs into
computational instability and higher cost the longer the sequence is, due to the $O(n^2)$
evaluation cost of De Casteljau's algorithm. We also design an additional architecture which
composes the previous architecture for many small $C^0$ curves. Fundamentally, this idea permits
for reconstruction of infinitely long sequences with guaranteed smoothness at little extra cost.

We define the architecture akin to poly-splines, or a composition of many small Bezier splines.
For a known-length sequence, we divide it into $K$ different segments, and without loss of
generality assume that $K$ evenly divides the total number of frames. This implies that $t$ is
now in the range $[0, K]$, and can be decomposed into a segment number and fractional component:
\[ k\in\mathbb{Z}_K=\lfloor t\rfloor, t'\in[0,1)=t-k \]
Then we define an embedding $\text{Emb} = \mathbb{R}^{Z\times(K+1)}$, where $Z$ is some latent
dimension size. We then create an "anchor" network:
\[
    \textit{anchor}(x\in\mathbb{R}^3,z\in\mathbb{R}^Z) = \text{MLP}(x,z)\to(\mathbb{R}^3,\mathbb{R}^{Z'})
\]
So named because it computes "anchors"
or endpoints of the curves, as well as a representative latent vector. Between these two
anchors, we are interested in computing some number of intermediate spline points. We define the
control point estimation network as
\[
\textit{control}(x\in\mathbb{R}^3, z_1, z_2\in\mathbb{R}^{Z'}) = \text{MLP}(x,z_1,z_2)\to\mathbb{R}^{O\times\mathbb{R}^3}
\]
Where $z_1,z_2$ are the
representative latent vectors from the anchor network, and it outputs $O$ spline points in
$R^3$. We optionally also can include a per-segment rigidity value, similar to above, but lose
some guarantees of continuity by doing so.

In order to evaluate the spline at a given time $t$ for a point $x$, we first compute $k, t'$. Then, we compute
the anchor points at
\[ p_0,z_1=\textit{anchor}(x,\text{Emb}[k]) \]
\[ p_\text{end},z_2=\textit{anchor}(x,\text{Emb}[k+1]) \]
And the middle control points as \[ p_{1\cdots O-2} = \text{control}(x,z_1,z_2) \]
The final set of control points we use
is the concatenation of the anchors with the intermediate points:
$[p_0, p_{1\cdots O-2}, p_\text{end}]$. Using this set of control points, evaluate the Bezier
curve spline at $t'$ using De Casteljau's algorithm:
\[ \text{De Casteljau}([p_0, p_{1\cdots O-2}, p_\text{end}], t) = \Delta(x) \]
Because we are using adjacent points from
the embedding for each anchor, we are guaranteed that the endpoints between each spline segment
are continuous. Information is also carried over between segments through the anchor's
representative latent vector, allowing the network to maintain velocity between curves if necessary.

This architecture enforces a guarantee of consistency, regardless of sequence length. It also
naturally allows for disentangling motion between distant time-steps, as while the endpoints of
the spline are fixed, movement in the middle is allowed to be fully independent of other
segments. This architecture is independent from dynamic NeRF, and we propose it for the broader task of generating $C^0$-continuous signals in any domain.

One may question as to why a more complex architecture is necessary as compared to just predicting all the spline points and using only a subset of them. In the case of extremely long sequences, we may expect that the number of spline points increases linearly with the sequence, and for dynamic scene reconstruction we would then have to sample $O(H\times W\times D\times t)$ at each point in space, even though we are only interested in a small subset. With this formulation, we are able to sample sparsely, maintaining a constant memory usage with respect to time.

We do not demonstrate the plausibility of this architecture, due to lack of data known to the
author of long videos with known camera positions, but hope to test it in future work\footnote{If this were to be published, I would demonstrate this architecture, but cannot due to time constraints.}.
